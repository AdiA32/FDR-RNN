{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "yG_n40gFzf9s"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHDoRoc5PKWz"
      },
      "source": [
        "### Scrape FDR Fireside chats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pD_55cOxLkAb",
        "outputId": "31e0f740-9c3c-48c4-de2c-cd95a8a42a7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraped speech from https://www.presidency.ucsb.edu/documents/fireside-chat-banking\n",
            "Scraped speech from https://www.presidency.ucsb.edu/documents/second-fireside-chat\n",
            "Scraped speech from https://www.presidency.ucsb.edu/documents/fireside-chat-recovery-program\n",
            "Scraped speech from https://www.presidency.ucsb.edu/documents/fireside-chat-22\n",
            "Scraped speech from https://www.presidency.ucsb.edu/documents/fireside-chat-21\n",
            "Scraped speech from https://www.presidency.ucsb.edu/documents/fireside-chat-20\n",
            "Scraped speech from https://www.presidency.ucsb.edu/documents/fireside-chat-19\n",
            "Scraped speech from https://www.presidency.ucsb.edu/documents/fireside-chat-18\n",
            "Scraped speech from https://www.presidency.ucsb.edu/documents/fireside-chat-17\n",
            "Scraped speech from https://www.presidency.ucsb.edu/documents/fireside-chat-16\n",
            "Scraped speech from https://www.presidency.ucsb.edu/documents/fireside-chat-15\n",
            "Scraped speech from https://www.presidency.ucsb.edu/documents/fireside-chat-14\n",
            "Scraped speech from https://www.presidency.ucsb.edu/documents/fireside-chat-13\n",
            "Scraped speech from https://www.presidency.ucsb.edu/documents/fireside-chat-10\n",
            "Scraped speech from https://www.presidency.ucsb.edu/documents/fireside-chat-9\n",
            "Scraped speech from https://www.presidency.ucsb.edu/documents/fireside-chat-11\n",
            "Scraped speech from https://www.presidency.ucsb.edu/documents/fireside-chat-12\n",
            "Scraped speech from https://www.presidency.ucsb.edu/documents/fireside-chat-6\n",
            "Scraped speech from https://www.presidency.ucsb.edu/documents/fireside-chat-5\n",
            "Scraped speech from https://www.presidency.ucsb.edu/documents/fireside-chat-7\n",
            "Scraped speech from https://www.presidency.ucsb.edu/documents/fireside-chat-4\n",
            "Scraped speech from https://www.presidency.ucsb.edu/documents/fireside-chat-0\n",
            "Scraped speech from https://www.presidency.ucsb.edu/documents/fireside-chat-1\n",
            "Scraped speech from https://www.presidency.ucsb.edu/documents/fireside-chat-8\n",
            "Scraped speech from https://www.presidency.ucsb.edu/documents/fireside-chat\n",
            "Scraped speech from https://www.presidency.ucsb.edu/documents/fireside-chat-2\n",
            "Scraped speech from https://www.presidency.ucsb.edu/documents/fireside-chat-3\n",
            "Total speeches scraped and stored: 27\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "\n",
        "def get_speech_links(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    speech_links = [a['href'] for a in soup.select('.views-field-title a')]\n",
        "    return ['https://www.presidency.ucsb.edu' + link for link in speech_links]\n",
        "\n",
        "def scrape_speech(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    text_blocks = soup.select('.field-docs-content p')\n",
        "    speech_text = '\\n'.join(block.text for block in text_blocks)  # Corrected variable name here\n",
        "    return speech_text\n",
        "\n",
        "def main():\n",
        "    base_url = \"https://www.presidency.ucsb.edu/advanced-search?field-keywords=&field-keywords2=&field-keywords3=&from%5Bdate%5D=&to%5Bdate%5D=&person2=200288&category2%5B%5D=53&items_per_page=100\"\n",
        "    links = get_speech_links(base_url)\n",
        "    all_speeches = []\n",
        "\n",
        "    for link in links:\n",
        "        try:\n",
        "            speech_text = scrape_speech(link)\n",
        "            all_speeches.append(speech_text)\n",
        "            print(f\"Scraped speech from {link}\")\n",
        "            time.sleep(1)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to scrape {link}: {e}\")\n",
        "\n",
        "    # we are going to just simply store the speeches in a text file because it will be easier for the RNN to be trained with.\n",
        "    with open('fireside_chats.txt', 'w', encoding='utf-8') as file:\n",
        "        file.write(\"\\n\\n\".join(all_speeches))\n",
        "\n",
        "    print(f\"Total speeches scraped and stored: {len(all_speeches)}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sMxyzlEGSDiI"
      },
      "outputs": [],
      "source": [
        "path_to_file = \"fireside_chats.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aavnuByVymwK",
        "outputId": "78bd5943-9a82-48d2-c63f-de4928e9dc20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of text: 459482 characters\n"
          ]
        }
      ],
      "source": [
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "print(f'Length of text: {len(text)} characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Duhg9NrUymwO",
        "outputId": "def68a7b-1edf-41e8-cb22-fb0807baccb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[First Fireside Chat]\n",
            "I want to talk for a few minutes with the people of the United States about bankingâ€”with the comparatively few who understand the mechanics of banking but more particularly with the overwhelming majority who use banks for the ma\n"
          ]
        }
      ],
      "source": [
        "print(text[:250])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlCgQBRVymwR",
        "outputId": "ba5cbb2e-a77a-47d1-cdd4-7ed6a81a8c99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "80 unique characters\n"
          ]
        }
      ],
      "source": [
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "6GMlCe3qzaL9"
      },
      "outputs": [],
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Wd2m3mqkDjRj"
      },
      "outputs": [],
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "w5apvBDn9Ind"
      },
      "outputs": [],
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UopbsKi88tm5",
        "outputId": "9f6fa8f3-e31b-4596-ee50-09da38a970f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(459482,), dtype=int64, numpy=array([50, 31, 61, ..., 70, 77,  3])>"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "qmxrYDCTy-eL"
      },
      "outputs": [],
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjH5v45-yqqH",
        "outputId": "a30add8a-2b43-43cc-8a77-f81933bb6744"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "F\n",
            "i\n",
            "r\n"
          ]
        }
      ],
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "C-G2oaTxy6km"
      },
      "outputs": [],
      "source": [
        "seq_length = 100\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpdjRO2CzOfZ",
        "outputId": "635239e4-cf38-4e2f-d1fa-934ac2c36a83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b'[' b'F' b'i' b'r' b's' b't' b' ' b'F' b'i' b'r' b'e' b's' b'i' b'd'\n",
            " b'e' b' ' b'C' b'h' b'a' b't' b']' b'\\n' b'I' b' ' b'w' b'a' b'n' b't'\n",
            " b' ' b't' b'o' b' ' b't' b'a' b'l' b'k' b' ' b'f' b'o' b'r' b' ' b'a'\n",
            " b' ' b'f' b'e' b'w' b' ' b'm' b'i' b'n' b'u' b't' b'e' b's' b' ' b'w'\n",
            " b'i' b't' b'h' b' ' b't' b'h' b'e' b' ' b'p' b'e' b'o' b'p' b'l' b'e'\n",
            " b' ' b'o' b'f' b' ' b't' b'h' b'e' b' ' b'U' b'n' b'i' b't' b'e' b'd'\n",
            " b' ' b'S' b't' b'a' b't' b'e' b's' b' ' b'a' b'b' b'o' b'u' b't' b' '\n",
            " b'b' b'a' b'n'], shape=(101,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QO32cMWu4a06",
        "outputId": "b2da834f-9395-4f82-a9f1-3e868f3f0a9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b'[First Fireside Chat]\\nI want to talk for a few minutes with the people of the United States about ban'\n",
            "b'king\\xe2\\x80\\x94with the comparatively few who understand the mechanics of banking but more particularly with th'\n",
            "b'e overwhelming majority who use banks for the making of deposits and the drawing of checks. I want to'\n",
            "b' tell you what has been done in the last few days, why it was done, and what the next steps are going'\n",
            "b' to be. I recognize that the many proclamations from State Capitols and from Washington, the legislat'\n"
          ]
        }
      ],
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "9NGu-FkO_kYU"
      },
      "outputs": [],
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "B9iKPXkw5xwa"
      },
      "outputs": [],
      "source": [
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNbw-iR0ymwj",
        "outputId": "7abc4e95-3b69-4025-c500-3f4720e643d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input : b'[First Fireside Chat]\\nI want to talk for a few minutes with the people of the United States about ba'\n",
            "Target: b'First Fireside Chat]\\nI want to talk for a few minutes with the people of the United States about ban'\n"
          ]
        }
      ],
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2pGotuNzf-S",
        "outputId": "d6bca53a-1b54-44ec-be52-ba5bb0be1c82"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajnI-Wda6nEB",
        "outputId": "da4a1966-cb36-46f3-b4b6-7cc7a1485dbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 30 Complete [00h 00m 43s]\n",
            "loss: 2.1551434993743896\n",
            "\n",
            "Best loss So Far: 0.9034488201141357\n",
            "Total elapsed time: 00h 12m 23s\n",
            "\n",
            "The optimal number of units in the GRU layer is 1024 and\n",
            "the optimal learning rate for the optimizer is 0.0026263512824869465.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import kerastuner as kt\n",
        "\n",
        "def build_model(hp):\n",
        "    vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "    embedding_dim = hp.Int('embedding_dim', min_value=32, max_value=512, step=32)\n",
        "    units = hp.Int('units', min_value=64, max_value=1024, step=64)\n",
        "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')\n",
        "\n",
        "    inputs = tf.keras.Input(shape=(None,))\n",
        "    x = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True)(inputs)\n",
        "    x, _ = tf.keras.layers.GRU(units, return_sequences=True, return_state=True)(x)\n",
        "    outputs = tf.keras.layers.Dense(vocab_size)(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "tuner = kt.Hyperband(\n",
        "    build_model,\n",
        "    objective='loss',\n",
        "    max_epochs=10,\n",
        "    directory='hyperband',\n",
        "    project_name='text_gen'\n",
        ")\n",
        "\n",
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
        "\n",
        "tuner.search(dataset, epochs=50, callbacks=[stop_early])\n",
        "\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(f\"\"\"\n",
        "The optimal number of units in the GRU layer is {best_hps.get('units')} and\n",
        "the optimal learning rate for the optimizer is {best_hps.get('learning_rate')}.\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "zHT8cLh7EAsg"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "embedding_dim = 256\n",
        "rnn_units = 768\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "wj8HQ2w8z4iO"
      },
      "outputs": [],
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "IX58Xj9z47Aw"
      },
      "outputs": [],
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-_70kKAPrPU",
        "outputId": "a6652f7c-6d2a-46c6-948d-942e01208bb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(64, 100, 81) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPGmAAXmVLGC",
        "outputId": "7a6b8e46-3763-41a1-a090-0d27243e82e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     multiple                  20736     \n",
            "                                                                 \n",
            " gru_1 (GRU)                 multiple                  3938304   \n",
            "                                                                 \n",
            " dense_1 (Dense)             multiple                  83025     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4042065 (15.42 MB)\n",
            "Trainable params: 4042065 (15.42 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "4V4MfFg0RQJg"
      },
      "outputs": [],
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqFMUQc_UFgM",
        "outputId": "2ca48d79-fb34-485a-99e1-14f935f1bd09"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([71, 55, 10, 24, 15, 14, 51, 42, 11, 65,  5, 24, 11,  7, 20,  6, 48,\n",
              "        5, 77, 14, 25, 20, 41, 74, 20,  7, 44, 31, 25, 61, 39,  1, 31, 33,\n",
              "       19, 63,  1, 28, 80, 40, 15, 77,  6, 46, 70, 39, 50, 70, 42, 71, 40,\n",
              "       38, 15, 29, 72, 50, 75, 47, 21, 48, 32, 32,  9, 68, 62, 67, 53, 33,\n",
              "       51, 52, 61, 73,  6, 73, 47, 35, 54, 31, 62, 19, 62,  3, 29,  9, 71,\n",
              "       71, 35, 51, 48, 56,  8, 38,  4, 75, 17, 49, 53, 45, 20,  4])"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sampled_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWcFwPwLSo05",
        "outputId": "f4987394-d390-424a-ad9b-550c0b1eda8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:\n",
            " b'for hope\\xe2\\x80\\x94hope for peace, yes, and hope for the defense of our civilization and for the building of a'\n",
            "\n",
            "Next Char Predictions:\n",
            " b'sc-;21]R.m$;.(7\\'Y$y1?7Pv7(TF?iN\\nFH6k\\nC\\xe2\\x80\\x94O2y\\'VrN[rRsOM2Dt[wW8YGG,pjoaH]_iu\\'uWJbFj6j!D,ssJ]Yd)M\"w4ZaU7\"'\n"
          ]
        }
      ],
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "ZOeWdgxNFDXq"
      },
      "outputs": [],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HrXTACTdzY-",
        "outputId": "2d6ccc5f-0067-4f48-9025-f00bd0a11845"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 81)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.3938093, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAJfS5YoFiHf",
        "outputId": "8225ea1a-5057-49d2-da8c-bf083c614756"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "80.9482"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "DDl1_Een6rL0"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "W6fWTriUZP-n"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "7yGBE2zxMMHs"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UK-hmKjYVoll",
        "outputId": "e68c08c1-8857-4c23-dd40-ccf4ad335e39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "71/71 [==============================] - 8s 70ms/step - loss: 3.0285\n",
            "Epoch 2/20\n",
            "71/71 [==============================] - 7s 67ms/step - loss: 2.2640\n",
            "Epoch 3/20\n",
            "71/71 [==============================] - 6s 64ms/step - loss: 2.0432\n",
            "Epoch 4/20\n",
            "71/71 [==============================] - 6s 62ms/step - loss: 1.8472\n",
            "Epoch 5/20\n",
            "71/71 [==============================] - 5s 62ms/step - loss: 1.6760\n",
            "Epoch 6/20\n",
            "71/71 [==============================] - 5s 63ms/step - loss: 1.5313\n",
            "Epoch 7/20\n",
            "71/71 [==============================] - 5s 63ms/step - loss: 1.4177\n",
            "Epoch 8/20\n",
            "71/71 [==============================] - 5s 63ms/step - loss: 1.3300\n",
            "Epoch 9/20\n",
            "71/71 [==============================] - 5s 61ms/step - loss: 1.2613\n",
            "Epoch 10/20\n",
            "71/71 [==============================] - 5s 61ms/step - loss: 1.2045\n",
            "Epoch 11/20\n",
            "71/71 [==============================] - 5s 59ms/step - loss: 1.1548\n",
            "Epoch 12/20\n",
            "71/71 [==============================] - 5s 61ms/step - loss: 1.1088\n",
            "Epoch 13/20\n",
            "71/71 [==============================] - 6s 60ms/step - loss: 1.0681\n",
            "Epoch 14/20\n",
            "71/71 [==============================] - 5s 61ms/step - loss: 1.0257\n",
            "Epoch 15/20\n",
            "71/71 [==============================] - 5s 61ms/step - loss: 0.9835\n",
            "Epoch 16/20\n",
            "71/71 [==============================] - 5s 62ms/step - loss: 0.9398\n",
            "Epoch 17/20\n",
            "71/71 [==============================] - 6s 62ms/step - loss: 0.8958\n",
            "Epoch 18/20\n",
            "71/71 [==============================] - 5s 63ms/step - loss: 0.8507\n",
            "Epoch 19/20\n",
            "71/71 [==============================] - 5s 61ms/step - loss: 0.8025\n",
            "Epoch 20/20\n",
            "71/71 [==============================] - 6s 69ms/step - loss: 0.7495\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKkD5M6eoSiN"
      },
      "source": [
        "## Generate text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "iSBU1tHmlUSs"
      },
      "outputs": [],
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    return predicted_chars, states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "fqMOuDutnOxK"
      },
      "outputs": [],
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ST7PSyk9t1mT",
        "outputId": "350dc572-a861-4bf8-b0ef-922871e232ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Good of the Assurance of the United States. The great vatual resources long ourselves with the oceans.\n",
            "The United Nations have been hopelessly if the heads there was always possible for awory of the German support my friends, but also lone and sha powerfulopeal manks and farms without warningly, I am doing.\n",
            "America fellows when the face of the Government. The Nazi onstrup billions who live on bodies. The consequences of the Republic. The Siccess of the Pacific; and fixence theanter, will be a furction of was farmers as a Nazi dollars more in all parks of thought, gavelt upon the third; there were machines and cities. The country wird out in private industries that we can have a construmed by against importance to our sovereignby, and well known, on the bases in the United States should continue and greater freedom, or recently were including our fight. But it should be found today.\n",
            "That is what we have record ahead, with the just that in taking that if the United States who are building our \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.299772262573242\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['Good '])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkLu7Y8UCMT7",
        "outputId": "a0046a45-7803-44db-87f0-083ba3c5d167"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor([b'State of the country; but un an officially would be greatly his first, to help the engineer or the American besine reptacts far well coordinated reforms there from the what its simply I want to complete its human rises be called from excessive crops but from course, soon have ritain that port, we are ask homes for the first time in pircuntur peodle\\xe2\\x80\\x94consider these factories.\\nToday we have known as well as disappeared by the initial forces of those crouds\\xe2\\x80\\x94where there are figured upon our mapply and similar springn--instea loans\\xe2\\x80\\x94and it is charged.\\nNo. In electing units of the world, it became constant regularls with the smallers of Rome and every work in years here are tere in the face of the peoples have appointed for attacks, have been reached the opportunily to meet its heavy ruined from underproving her the s.\\nEverying superiority.\\nWhen years, as we have carefully in the farmer and the nettern.\\nWe also also our soldiers, sailors who were all my out of Federal Judges in cities that I wish to assing its as'], shape=(1,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.5319766998291016\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['The taxes'])\n",
        "next_char = tf.constant(['State of the country'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Grk32H_CzsC",
        "outputId": "3647dea1-4e25-4607-906c-73cd0eac55f1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x79184d5ca5c0>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ],
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Z9bb_wX6Uuu",
        "outputId": "62d9ec31-dedc-4b91-cc5f-cd99d05d328b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Good raise and sound labor.\n",
            "Six men to take those madble with us to the Congress that the pircussition o\n"
          ]
        }
      ],
      "source": [
        "states = None\n",
        "next_char = tf.constant(['Good'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JM3_Z12fI7mN"
      },
      "source": [
        "Huge shoutout to tensorflow.org for helping with code and providing such awesome tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAIYo9TrSV-g"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
